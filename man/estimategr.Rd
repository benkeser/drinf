% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimateGr.R
\name{estimategr}
\alias{estimategr}
\title{estimategr}
\usage{
estimategr(rg0, rg1, g0n, g1n, A0, A1, Q2n, Q1n, SL.gr, abar, return.models,
  tolg, verbose, ...)
}
\arguments{
\item{rg0}{The "residual" for the first reduced dimension regression (on Q1n).}

\item{rg1}{The "residual" for the second reduced dimension regression (on Q2n).}

\item{g0n}{A \code{vector} of estimates of g_{0,0}}

\item{g1n}{A \code{vector} of estimates of g_{1,0}}

\item{A0}{A \code{vector} treatment delivered at baseline.}

\item{A1}{A \code{vector} treatment deliver after \code{L1} is measured.}

\item{Q2n}{A \code{vector} of estimates of Q_{2,0}}

\item{Q1n}{A \code{vector} of estimates of Q_{1,0}}

\item{SL.gr}{A \code{vector} or \code{list} specifying the SuperLearner library
to be used to estimate the reduced-dimension regression to protect against misspecification of the
treatment regressions.  See \code{SuperLearner} package for details.}

\item{abar}{A \code{vector} of length 2 indicating the treatment assignment 
that is of interest.}

\item{return.models}{A \code{boolean} indicating whether the models for Qr0 should be 
returned with the output.}

\item{tolg}{A \code{numeric} indicating the truncation level for conditional treatment probabilities.}
}
\value{
A list with elements g0nr, g1nr, h0nr, h1nr, and hbarnr, corresponding to the
predicted values of the reduced dimension regressions. Also included in output are the
models used to obtain these predicted values (set to \code{NULL} if \code{return.models = FALSE})
}
\description{
A function used to estimate the reduced dimension regressions for g. The regression 
can be computed using a user specified function, passed through \code{SL.gr} or using
\code{SuperLearner} when \code{length(SL.gr) == 1} or \code{is.list(SL.gr)}. There is 
an error proofing of the \code{SuperLearner} implementation that deals with situations where
the \code{NNLS} procedure in the Super Learner ensemble fails and so the function returns 
zero weights for every coefficient. In this case, the code will default to using the discrete
Super Learner; that is, the learner with lowest CV-risk.
}
